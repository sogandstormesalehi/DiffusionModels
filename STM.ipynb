{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f_NSaSoYpxnb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialTransformer(nn.Module):\n",
        "  def __init__(self, channels, n_heads, n_layers, d_cond):\n",
        "    super().__init__()\n",
        "    self.norm = torch.nn.GroupNorm(num_groups = 32, num_channels = channels, eps = 1e-5, affine = True)\n",
        "    self.proj_in = nn.Conv2d(channels, channels, kernel_size = 1, stride = 1, padding = 0)\n",
        "    self.transformer_blocks = nn.ModuleList(\n",
        "        [BasicTransformerBlock(channels, n_heads, channels // n_heads, d_cond = d_cond) for i in range(n_layers)]\n",
        "    )\n",
        "    self.proj_out = nn.Conv2d(channels, channels, kernel_size = 1, stride = 1, padding = 0)\n",
        "  \n",
        "  def forward(self, x, cond):\n",
        "    batch_size, channels, height, width = x.shape\n",
        "    x_residual = x\n",
        "    x = self.norm(x)\n",
        "    x = self.proj_in(x)\n",
        "    x = x.permute(0, 2, 3, 1).view(batch_size, height * width, channels)\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(x, cond)\n",
        "    x = x.view(batch_size, height, width, channels).permute(0, 3, 1, 2)\n",
        "    x = self.proj_out(x)\n",
        "    return x + x_residual"
      ],
      "metadata": {
        "id": "a9Ejne1Pp7TX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicTransformerBlock(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_head, d_cond):\n",
        "    super().__init__()\n",
        "    self.attention1 = CrossAttention(d_model, d_model, n_heads, d_head)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.attention2 = CrossAttention(d_model, d_cond, n_heads, d_head)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.feedforward = FeedForward(d_model)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, cond):\n",
        "    x = self.attention1(self.norm1(x)) + x # self attention\n",
        "    x = self.attention2(self.norm2(x), cond = cond) + x # cross-attention conditioning\n",
        "    x = self.feedforward(self.norm3(x)) + x\n",
        "    return x"
      ],
      "metadata": {
        "id": "GfzMvZhks179"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "  def __init__(self, d_model, d_cond, n_heads, d_head, inplace = True):\n",
        "    super().__init__()\n",
        "    self.inplace = inplace\n",
        "    self.n_heads = n_heads\n",
        "    self.d_head = d_head\n",
        "    self.scale = d_head ** -0.5\n",
        "    d_attention = d_head * n_heads\n",
        "    self.q_mapping = nn.Linear(d_model, d_attention, bias = False)\n",
        "    self.k_mapping = nn.Linear(d_cond, d_attention, bias = False)\n",
        "    self.v_mapping = nn.Linear(d_cond, d_attention, bias = False)\n",
        "    self.out = nn.Sequential(nn.Linear(d_attention, d_model))\n",
        "    \n",
        "  def forward(self, x, cond = None):\n",
        "    if cond is None:\n",
        "      cond = x\n",
        "    q = self.q_mapping(x)\n",
        "    k = self.k_mapping(cond)\n",
        "    v = self.v_mapping(cond)\n",
        "    return self.normal_attention(q, k, v)\n",
        "\n",
        "  def normal_attention(self, q, k, v):\n",
        "    q = q.view(*q.shape[:2], self.n_heads, -1)\n",
        "    k = k.view(*k.shape[:2], self.n_heads, -1)\n",
        "    v = v.view(*v.shape[:2], self.n_heads, -1)\n",
        "    attention = torch.einsum('bihd,bjhd->bhij', q, k) * self.scale\n",
        "    if self.inplace:\n",
        "      half = attention.shape[0] // 2\n",
        "      attention[half:] = attention[half:].softmax(dim=-1)\n",
        "      attention[:half] = attention[:half].softmax(dim=-1)\n",
        "    else:\n",
        "      attention = attention.softmax(dim=-1)\n",
        "    \n",
        "    out = torch.einsum('bhij,bjhd->bihd', attention, v).reshape(*out.shape[:2], -1)\n",
        "    return self.out(out)"
      ],
      "metadata": {
        "id": "S2zrbcUeuHp8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_mult = 4):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        GeGLU(d_model, d_model * d_mult),\n",
        "        nn.Dropout(0.),\n",
        "        nn.Linear(d_model * d_mult, d_model)   \n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.net(x)"
      ],
      "metadata": {
        "id": "vbVtuhaj_6Jg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeGLU(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_in, d_out * 2)\n",
        "  def forward(self, x):\n",
        "    x, gate = self.proj(x).chunk(2, dim = -1)\n",
        "    return x * F.gelu(gate)"
      ],
      "metadata": {
        "id": "_i5GbNDRBPE1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hzBKn4bDB6j8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}