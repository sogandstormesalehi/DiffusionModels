{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2GP9wEogFay",
        "outputId": "919ff301-6a6f-401e-d2f2-081bc52ac734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting labml_nn\n",
            "  Downloading labml_nn-0.4.133-py3-none-any.whl (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting labml-helpers>=0.4.89\n",
            "  Downloading labml_helpers-0.4.89-py3-none-any.whl (24 kB)\n",
            "Collecting labml>=0.4.158\n",
            "  Downloading labml-0.4.161-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from labml_nn) (0.14.1+cu116)\n",
            "Collecting fairscale\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.8/dist-packages (from labml_nn) (0.14.1)\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from labml_nn) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from labml_nn) (1.22.4)\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from labml>=0.4.158->labml_nn) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->labml_nn) (4.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext->labml_nn) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext->labml_nn) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->labml_nn) (8.4.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext->labml_nn) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext->labml_nn) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext->labml_nn) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext->labml_nn) (2.10)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332138 sha256=06019ff48ec5fb517010f653e55e8541cd1a92237284e1f29eafc675220b7a7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/02/9b/dc7d4ff5145afdd28f456dae6605a46619af0370eca30d8d7e\n",
            "Successfully built fairscale\n",
            "Installing collected packages: smmap, einops, gitdb, fairscale, gitpython, labml, labml-helpers, labml_nn\n",
            "Successfully installed einops-0.6.0 fairscale-0.4.13 gitdb-4.0.10 gitpython-3.1.31 labml-0.4.161 labml-helpers-0.4.89 labml_nn-0.4.133 smmap-5.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install labml_nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Optional, Tuple, Union, List\n",
        "import torch\n",
        "from torch import nn\n",
        "from labml_helpers.module import Module"
      ],
      "metadata": {
        "id": "JdUQ2vDJz0gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Swish(Module):\n",
        "  def forward(self, x):#x⋅σ(x)\n",
        "    return x * torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "ewtfzC9m5P2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeEmbedding(nn.Module):\n",
        "\n",
        "  def __init__(self, n_channels):\n",
        "    super().__init__()\n",
        "    self.n_channels = n_channels\n",
        "    self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
        "    self.act = Swish()\n",
        "    self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
        "\n",
        "  def forward(self, t: torch.Tensor):\n",
        "    half_dim = self.n_channels // 8\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, device = t.device) * -emb)\n",
        "    emb = t[:, None] * emb[None, :]\n",
        "    print(t[:, None])\n",
        "    print(emb[None, :])\n",
        "    emb = torch.cat((emb.sin(), emb.cos()), dim = 1)\n",
        "    emb = self.act(self.lin1(emb))\n",
        "    emb = self.lin2(emb)\n",
        "    return emb"
      ],
      "metadata": {
        "id": "Qpv7aU6f-wG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(Module): # two convolution layers with group normalization\n",
        "  def __init__(self, in_channels, out_channels, time_channels, n_groups = 32, dropout = 0.1): # n_groups = the number of groups for group normalization\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
        "    self.activation1 = Swish()\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = (3, 3), padding=(1, 1))\n",
        "    self.norm1 = nn.GroupNorm(n_groups, out_channels)\n",
        "    self.activation2 = Swish()\n",
        "    self.conv1 = nn.Conv2d(out_channels, out_channels, kernel_size = (3, 3), padding=(1, 1))\n",
        "    if in_channels != out_channels:\n",
        "      self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
        "    else:\n",
        "      self.shortcut = nn.Identity()\n",
        "    self.time_emb = nn.Linear(time_channels, out_channels)\n",
        "    self.time_activation = Swish()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    x_copy = x.copy()\n",
        "    x = self.norm1(x)\n",
        "    x = self.activation1(x)\n",
        "    x = self.conv1(x)\n",
        "    x += self.time_emb(self.time_act(t))[:, :, None, None] # ???\n",
        "    print(self.time_emb(self.time_act(t))[:, :, None, None])\n",
        "    x = self.norm2(x)\n",
        "    x = self.activation2(x)\n",
        "    x = self.conv1(x)\n",
        "    print(x)\n",
        "    print(x + self.shortcut(x))\n",
        "    return x + self.shortcut(x)"
      ],
      "metadata": {
        "id": "L5P25GfeFn1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(Module):\n",
        "  def __init__(self, n_channels, n_heads, d_k, n_groups):\n",
        "    super().__init__()\n",
        "    if d_k is not None:\n",
        "      d_k = n_channels\n",
        "    self.norm = nn.GroupNorm(n_groups, n_channels)\n",
        "    self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
        "    self.output = nn.Linear(n_heads * d_k, n_channels)\n",
        "    self.scale = d_k ** -0.5\n",
        "    self.n_heads = n_heads\n",
        "    self.d_k = d_k\n",
        "\n",
        "  def forward(self, x, t = None): # t.shape: batch_size, time_channels\n",
        "    batch_size, n_channels, height, width = x.shape\n",
        "    x = x.view(batch_size, n_channels, -1).permute(0, 2, 1) # batch_size, seq, n_channels\n",
        "    query_key_value = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k) \n",
        "    query, key, value = torch.chunk(query_key_value, 3, dim = -1)\n",
        "    attention = torch.einsum('bihd, bjhd -> bijh', query, key) * self.scale\n",
        "    attention = attention.softmax(dim = 2)\n",
        "    result = torch.einsum('bijh, bjhd -> bihd', attention, value) # multiplying by values\n",
        "    result = result.view(batch_size, -1, self.n_heads * self.d_k)\n",
        "    result = self.output(result)\n",
        "    result += x # skip\n",
        "    result = result.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
        "    return result"
      ],
      "metadata": {
        "id": "sewgeBEla2BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownBlock(Module):\n",
        "  def __init__(self, in_channels, out_channels, time_channels, attention):\n",
        "    super().__init__()\n",
        "    self.residual = ResidualBlock(in_channels, out_channels, time_channels)\n",
        "    if attention:\n",
        "      self.attention = AttentionBlock(out_channels)\n",
        "    else:\n",
        "      self.attention = nn.Identity()\n",
        "    \n",
        "  def forward(self, x, t):\n",
        "    x = self.residual(x, t)\n",
        "    x = self.attention(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "4gqPkDWfh7ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlock(Module):\n",
        "  def __init__(self, in_channels, out_channels, time_channels, attention):\n",
        "    super().__init__()\n",
        "    self.residual = ResidualBlock(in_channels + out_channels, out_channels, time_channels)\n",
        "    if attention:\n",
        "      self.attention = AttentionBlock(out_channels)\n",
        "    else:\n",
        "      self.attention = nn.Identity()\n",
        "    \n",
        "  def forward(self, x, t):\n",
        "    x = self.residual(x, t)\n",
        "    x = self.attention(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "wBQ1yrGarxLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiddleBlock(Module):\n",
        "  def __init__(self, n_channels, time_channels):\n",
        "    super().__init__()\n",
        "    self.first_res = ResidualBlock(n_channels, n_channels, time_channels)\n",
        "    self.attention = AttentionBlock(n_channels)\n",
        "    self.second_res = ResidualBlock(n_channels, n_channels, time_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "    x = self.first_res(x, t)\n",
        "    x = self.attention(x)\n",
        "    x = self.second_res(x, t)\n",
        "    return x"
      ],
      "metadata": {
        "id": "KudyIaM9sTUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Upsample(nn.Module):\n",
        "  def __init__(self, n_channels):\n",
        "    super().__init__()\n",
        "    self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    return self.conv(x)"
      ],
      "metadata": {
        "id": "A7O01l1rs4kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Downsample(nn.Module):\n",
        "  def __init__(self, n_channels):\n",
        "    super().__init__()\n",
        "    self.conv = nn.ConvTranspose2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    return self.conv(x)"
      ],
      "metadata": {
        "id": "Cyhzwfrdv4ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(Module):\n",
        "  def __init__(self, image_channels = 3, n_channels = 64, channels_per_res = (1, 2, 2, 4), has_attention = (False, False, True, True), n_blocks = 2):\n",
        "    super().__init__()\n",
        "    n_resolutions = len(channels_per_res)\n",
        "    self.image_projection = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "    self.time_embedding = TimeEmbedding(n_channels * 4)\n",
        "    down = []\n",
        "    out_channels = in_channels = n_channels\n",
        "    for i in range(n_resolutions):\n",
        "      out_channels = in_channels * channels_per_res[i]\n",
        "      for block in range(n_blocks):\n",
        "        down.append(DownBlock(in_channels, out_channels, n_channels * 4, has_attention[i]))\n",
        "        in_channels = out_channels\n",
        "      if i < n_resolutions - 1:\n",
        "        down.append(Downsample(in_channels))\n",
        "    self.down = nn.ModuleList(down)\n",
        "\n",
        "    self.middle = MiddleBlock(out_channels, n_channels * 4, )\n",
        "\n",
        "    up = []\n",
        "    in_channels = out_channels\n",
        "    for i in reversed(range(n_resolutions)):\n",
        "      out_channels = in_channels\n",
        "      for block in range(n_blocks):\n",
        "        up.append(UpBlock(in_channels, out_channels, n_channels * 4, has_attention[i]))\n",
        "      out_channels = in_channels // channels_per_res[i]\n",
        "      up.append(UpBlock(in_channels, out_channels, n_channels * 4, has_attention[i]))\n",
        "      in_channels = out_channels\n",
        "      if i > 0:\n",
        "        up.append(Upsample(in_channels))\n",
        "    self.up = nn.ModuleList(up)\n",
        "\n"
      ],
      "metadata": {
        "id": "Kfpeem7uwNwY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}